# -*- coding: utf-8 -*-
"""Copy of cwru-evaluation-by-severity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Kq0UH2FCLkMNe0lWXBbx3fGEmqMX9Dh

# CWRU files.

Associate each Matlab file name to a bearing condition in a Python dictionary.
The dictionary keys identify the conditions.

There are only four normal conditions, with loads of 0, 1, 2 and 3 hp.
All conditions end with an underscore character followed by an algarism representing the load applied during the acquisitions.
The remaining conditions follow the pattern:


* First two characters represent the bearing location, i.e. drive end (DE) and fan end (FE).
* The following two characters represent the failure location in the bearing, i.e. ball (BA), Inner Race (IR) and Outer Race (OR).
* The next three algarisms indicate the severity of the failure, where 007 stands for 0.007 inches and 0021 for 0.021 inches.
* For Outer Race failures, the character @ is followed by a number that indicates different load zones.
"""

debug = False
# size of each segment
sample_size = 32768
if not debug:
  sample_size = 512
acquisitions = {}
# Normal
acquisitions["Normal_0"] = "97.mat"
acquisitions["Normal_1"] = "98.mat"
acquisitions["Normal_2"] = "99.mat"
acquisitions["Normal_3"] = "100.mat"
# DE Inner Race 0.007 inches
acquisitions["DEIR.007_0"] = "105.mat"
acquisitions["DEIR.007_1"] = "106.mat"
acquisitions["DEIR.007_2"] = "107.mat"
acquisitions["DEIR.007_3"] = "108.mat"
# DE Ball 0.007 inches
acquisitions["DEB.007_0"] = "118.mat"
acquisitions["DEB.007_1"] = "119.mat"
acquisitions["DEB.007_2"] = "120.mat"
acquisitions["DEB.007_3"] = "121.mat"
# DE Outer race 0.007 inches centered @6:00
acquisitions["DEOR.007@6_0"] = "130.mat"
acquisitions["DEOR.007@6_1"] = "131.mat"
acquisitions["DEOR.007@6_2"] = "132.mat"
acquisitions["DEOR.007@6_3"] = "133.mat"
# DE Outer race 0.007 inches centered @3:00
acquisitions["DEOR.007@3_0"] = "144.mat"
acquisitions["DEOR.007@3_1"] = "145.mat"
acquisitions["DEOR.007@3_2"] = "146.mat"
acquisitions["DEOR.007@3_3"] = "147.mat"
# DE Outer race 0.007 inches centered @12:00
acquisitions["DEOR.007@12_0"] = "156.mat"
acquisitions["DEOR.007@12_1"] = "158.mat"
acquisitions["DEOR.007@12_2"] = "159.mat"
acquisitions["DEOR.007@12_3"] = "160.mat"
# DE Inner Race 0.014 inches
acquisitions["DEIR.014_0"] = "169.mat"
acquisitions["DEIR.014_1"] = "170.mat"
acquisitions["DEIR.014_2"] = "171.mat"
acquisitions["DEIR.014_3"] = "172.mat"
# DE Ball 0.014 inches
acquisitions["DEB.014_0"] = "185.mat"
acquisitions["DEB.014_1"] = "186.mat"
acquisitions["DEB.014_2"] = "187.mat"
acquisitions["DEB.014_3"] = "188.mat"
# DE Outer race 0.014 inches centered @6:00
acquisitions["DEOR.014@6_0"] = "197.mat"
acquisitions["DEOR.014@6_1"] = "198.mat"
acquisitions["DEOR.014@6_2"] = "199.mat"
acquisitions["DEOR.014@6_3"] = "200.mat"
# DE Ball 0.021 inches
acquisitions["DEB.021_0"] = "222.mat"
acquisitions["DEB.021_1"] = "223.mat"
acquisitions["DEB.021_2"] = "224.mat"
acquisitions["DEB.021_3"] = "225.mat"
# FE Inner Race 0.021 inches
acquisitions["FEIR.021_0"] = "270.mat"
acquisitions["FEIR.021_1"] = "271.mat"
acquisitions["FEIR.021_2"] = "272.mat"
acquisitions["FEIR.021_3"] = "273.mat"
# FE Inner Race 0.014 inches
acquisitions["FEIR.014_0"] = "274.mat"
acquisitions["FEIR.014_1"] = "275.mat"
acquisitions["FEIR.014_2"] = "276.mat"
acquisitions["FEIR.014_3"] = "277.mat"
# FE Ball 0.007 inches
acquisitions["FEB.007_0"] = "282.mat"
acquisitions["FEB.007_1"] = "283.mat"
acquisitions["FEB.007_2"] = "284.mat"
acquisitions["FEB.007_3"] = "285.mat"
# DE Inner Race 0.021 inches
acquisitions["DEIR.021_0"] = "209.mat"
acquisitions["DEIR.021_1"] = "210.mat"
acquisitions["DEIR.021_2"] = "211.mat"
acquisitions["DEIR.021_3"] = "212.mat"
# DE Outer race 0.021 inches centered @6:00
acquisitions["DEOR.021@6_0"] = "234.mat"
acquisitions["DEOR.021@6_1"] = "235.mat"
acquisitions["DEOR.021@6_2"] = "236.mat"
acquisitions["DEOR.021@6_3"] = "237.mat"
# DE Outer race 0.021 inches centered @3:00
acquisitions["DEOR.021@3_0"] = "246.mat"
acquisitions["DEOR.021@3_1"] = "247.mat"
acquisitions["DEOR.021@3_2"] = "248.mat"
acquisitions["DEOR.021@3_3"] = "249.mat"
# DE Outer race 0.021 inches centered @12:00
acquisitions["DEOR.021@12_0"] = "258.mat"
acquisitions["DEOR.021@12_1"] = "259.mat"
acquisitions["DEOR.021@12_2"] = "260.mat"
acquisitions["DEOR.021@12_3"] = "261.mat"
# FE Inner Race 0.007 inches
acquisitions["FEIR.007_0"] = "278.mat"
acquisitions["FEIR.007_1"] = "279.mat"
acquisitions["FEIR.007_2"] = "280.mat"
acquisitions["FEIR.007_3"] = "281.mat"
# FE Ball 0.014 inches
acquisitions["FEB.014_0"] = "286.mat"
acquisitions["FEB.014_1"] = "287.mat"
acquisitions["FEB.014_2"] = "288.mat"
acquisitions["FEB.014_3"] = "289.mat"
# FE Ball 0.021 inches
acquisitions["FEB.021_0"] = "290.mat"
acquisitions["FEB.021_1"] = "291.mat"
acquisitions["FEB.021_2"] = "292.mat"
acquisitions["FEB.021_3"] = "293.mat"
# FE Outer race 0.007 inches centered @6:00
acquisitions["FEOR.007@6_0"] = "294.mat"
acquisitions["FEOR.007@6_1"] = "295.mat"
acquisitions["FEOR.007@6_2"] = "296.mat"
acquisitions["FEOR.007@6_3"] = "297.mat"
# FE Outer race 0.007 inches centered @3:00
acquisitions["FEOR.007@3_0"] = "298.mat"
acquisitions["FEOR.007@3_1"] = "299.mat"
acquisitions["FEOR.007@3_2"] = "300.mat"
acquisitions["FEOR.007@3_3"] = "301.mat"
# FE Outer race 0.007 inches centered @12:00
acquisitions["FEOR.007@12_0"] = "302.mat"
acquisitions["FEOR.007@12_1"] = "305.mat"
acquisitions["FEOR.007@12_2"] = "306.mat"
acquisitions["FEOR.007@12_3"] = "307.mat"
# FE Outer race 0.014 inches centered @3:00
acquisitions["FEOR.014@3_0"] = "310.mat"
acquisitions["FEOR.014@3_1"] = "309.mat"
acquisitions["FEOR.014@3_2"] = "311.mat"
acquisitions["FEOR.014@3_3"] = "312.mat"
# FE Outer race 0.014 inches centered @6:00
acquisitions["FEOR.014@6_0"] = "313.mat"
# FE Outer race 0.021 inches centered @6:00
acquisitions["FEOR.021@6_0"] = "315.mat"
# FE Outer race 0.021 inches centered @3:00
acquisitions["FEOR.021@3_1"] = "316.mat"
acquisitions["FEOR.021@3_2"] = "317.mat"
acquisitions["FEOR.021@3_3"] = "318.mat"
# DE Inner Race 0.028 inches
acquisitions["DEIR.028_0"] = "3001.mat"
acquisitions["DEIR.028_1"] = "3002.mat"
acquisitions["DEIR.028_2"] = "3003.mat"
acquisitions["DEIR.028_3"] = "3004.mat"
# DE Ball 0.028 inches
acquisitions["DEB.028_0"] = "3005.mat"
acquisitions["DEB.028_1"] = "3006.mat"
acquisitions["DEB.028_2"] = "3007.mat"
acquisitions["DEB.028_3"] = "3008.mat"

"""#Functions definitions"""

def get_labels_dict(acquisitions, separator='_', detectPosition=True):
  """Generate a dictionary linking the labels with values to keep consistence."""
  labels_dict = {}
  value = 0
  for key in acquisitions.keys():
    key = key.split('_')[0]
    key = key.split(separator)
    if key[0] == "Normal" or detectPosition:
      label = key[0]
    else:
      label = key[0][2:]
    if not label in labels_dict:
      labels_dict[label] = value
      value += 1
  return labels_dict

"""Convert Matlab file into tensors."""

import scipy.io
import numpy as np
def acquisition2tensor(file_name, position=None, sample_size=sample_size):
  """
  Convert Matlab file into tensors.
  The file is divided in segments of sample_size values.
  """
  print(file_name, end=' ')
  matlab_file = scipy.io.loadmat(file_name)
  DE_samples = []
  FE_samples = []
  
  #signal segmentation
  signal_begin = 0
  if position == None:
    DE_time = [key for key in matlab_file if key.endswith("DE_time")][0] #Find the DRIVE END acquisition key name
    FE_time = [key for key in matlab_file if key.endswith("FE_time")][0] #Find the FAN END acquisition key name
    acquisition_size = max(len(matlab_file[DE_time]),len(matlab_file[FE_time]))
    while signal_begin + sample_size < acquisition_size:
      DE_samples.append([item for sublist in matlab_file[DE_time][signal_begin:signal_begin+sample_size] for item in sublist])
      FE_samples.append([item for sublist in matlab_file[FE_time][signal_begin:signal_begin+sample_size] for item in sublist])
      signal_begin += sample_size
    sample_tensor = np.stack([DE_samples,FE_samples],axis=2).astype('float32')
  elif position == 'DE':
    DE_time = [key for key in matlab_file if key.endswith("DE_time")][0] #Find the DRIVE END acquisition key name
    acquisition_size = len(matlab_file[DE_time])
    while signal_begin + sample_size < acquisition_size:
      DE_samples.append([item for sublist in matlab_file[DE_time][signal_begin:signal_begin+sample_size] for item in sublist])
      signal_begin += sample_size
    sample_tensor = np.stack([DE_samples],axis=2).astype('float32')
  elif position == 'FE':
    FE_time = [key for key in matlab_file if key.endswith("FE_time")][0] #Find the FAN END acquisition key name
    acquisition_size = len(matlab_file[FE_time])
    while signal_begin + sample_size < acquisition_size:
      FE_samples.append([item for sublist in matlab_file[FE_time][signal_begin:signal_begin+sample_size] for item in sublist])
      signal_begin += sample_size
    sample_tensor = np.stack([FE_samples],axis=2).astype('float32')
  return sample_tensor

"""Extract datasets from acquisitions."""

def concatenate_datasets(xd,yd,xo,yo):
  """
  xd: destination patterns tensor
  yd: destination labels tensor
  xo: origin patterns tensor to be concateneted 
  yo: origin labels tensor to be concateneted 
  """
  if xd is None or yd is None:
    xd = xo
    yd = yo
  else:
    xd = np.concatenate((xd,xo))
    yd = np.concatenate((yd,yo))
  return xd,yd

import urllib.request
import os.path

def acquisitions_from_substr(substr, acquisitions, labels_dict, position=None,
                             url="http://csegroups.case.edu/sites/default/files/bearingdatacenter/files/Datafiles/"):
  """
  Extract samples from all files with some load.
  """
  samples = None
  labels = None
  for key in acquisitions:
    if str(substr) in key:
      file_name = acquisitions[key]
      if not os.path.exists(file_name):
        urllib.request.urlretrieve(url+file_name, file_name)
      if substr[:2] == key[:2] and position == None:
        acquisition_samples = acquisition2tensor(file_name)
      elif position =='DE':
        acquisition_samples = acquisition2tensor(file_name, 'DE')
      elif position =='FE':
        acquisition_samples = acquisition2tensor(file_name, 'FE')
      else:
        acquisition_samples = acquisition2tensor(file_name, key[:2])
      for label in labels_dict.keys():
        if label in key:
          break
      acquisition_labels = np.ones(acquisition_samples.shape[0])*labels_dict[label]
      samples,labels = concatenate_datasets(samples,labels,acquisition_samples,acquisition_labels)
  print(substr)
  return samples,labels

"""Define function to plot the confusion matrices."""

import itertools
from sklearn.metrics import confusion_matrix
from google.colab import files
from matplotlib import pyplot as plt


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Greys):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    #plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')

"""#Downloading and Matlab files
Extract samples.
"""

labels_dict = get_labels_dict(acquisitions, '.', False)
print(labels_dict)
def normal_indenpendent_position_acquisitions(load,acquisitions,labels_dict):
  x,y = None,None
  for position in ['DE','FE']:
    xn,yn = acquisitions_from_substr('Normal_'+str(load),acquisitions,labels_dict,position)
    x,y = concatenate_datasets(x,y,xn,yn)
  return x,y

xn_0,yn_0 = normal_indenpendent_position_acquisitions(0,acquisitions,labels_dict)
xn_1,yn_1 = normal_indenpendent_position_acquisitions(1,acquisitions,labels_dict)
xn_2,yn_2 = normal_indenpendent_position_acquisitions(2,acquisitions,labels_dict)
xn_3,yn_3 = normal_indenpendent_position_acquisitions(3,acquisitions,labels_dict)

x007,y007 = acquisitions_from_substr('007',acquisitions,labels_dict)
x014,y014 = acquisitions_from_substr('014',acquisitions,labels_dict)
x021,y021 = acquisitions_from_substr('021',acquisitions,labels_dict)
x028,y028 = acquisitions_from_substr('028',acquisitions,labels_dict)

severities = ['007','014','021','028']

"""Count number of samples."""

print("Label", end='\t')
for s in severities:
  print(s, end='\t')
print("total")
mat = np.zeros((4,4))
i = 0
for label,value in labels_dict.items():
  print(label, end='\t')
  tsamples = 0
  if label == 'Normal':
    print(4*'\t'+'...')
    for load in range(4):
      print(' '+str((load+len(severities)+1)%4)+(load)*'\t', end='\t')
      mat[i][load] = list(eval('yn_'+str((load+len(severities)+1)%4))).count(value)
      print(int(mat[i][load]))
  else:
    for j,severity in enumerate(['007','014','021','028']):
      tmp = eval('y'+str(severity))
      if tmp is not None:
        nsamples = list(tmp).count(value)
        mat[i][j] = nsamples
        print(nsamples, end='\t')
        tsamples += nsamples
      else:
        print('0', end='\t')
    print(tsamples)
  i+=1
total = np.sum(mat,axis=0)
print("Total:", end='\t')
for i in range(len(total)):
  print(int(total[i]), end='\t')
print(int(np.sum(total)))

"""#Feature Extraction Models"""

from sklearn.base import TransformerMixin
import numpy as np
import scipy.stats as stats

# roor mean square
def rms(x):
  x = np.array(x)
  return np.sqrt(np.mean(np.square(x)))
# square root amplitude
def sra(x):
  x = np.array(x)
  return np.mean(np.sqrt(np.absolute(x)))**2
# peak to peak value
def ppv(x):
  x = np.array(x)
  return np.max(x)-np.min(x)
# crest factor
def cf(x):
  x = np.array(x)
  return np.max(np.absolute(x))/rms(x)
# impact factor
def ifa(x):
  x = np.array(x)
  return np.max(np.absolute(x))/np.mean(np.absolute(x))
# margin factor
def mf(x):
  x = np.array(x)
  return np.max(np.absolute(x))/sra(x)
# shape factor
def sf(x):
  x = np.array(x)
  return rms(x)/np.mean(np.absolute(x))
# kurtosis factor
def kf(x):
  x = np.array(x)
  return stats.kurtosis(x)/(np.mean(x**2)**2)

class StatisticalTime(TransformerMixin):
  def __init__(self):
    pass
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    if X.shape[2] == 1:
      return np.array([[rms(x), sra(x), stats.kurtosis(x), stats.skew(x), ppv(x), cf(x), ifa(x), mf(x), sf(x), kf(x)] for x in X[:,:,0]])
    de = np.array([[rms(x), sra(x), stats.kurtosis(x), stats.skew(x), ppv(x), cf(x), ifa(x), mf(x), sf(x), kf(x)] for x in X[:,:,0]])
    fe = np.array([[rms(x), sra(x), stats.kurtosis(x), stats.skew(x), ppv(x), cf(x), ifa(x), mf(x), sf(x), kf(x)] for x in X[:,:,1]])
    return np.concatenate((de,fe),axis=1)
  
class StatisticalFrequency(TransformerMixin):
  def __init__(self):
    pass
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    if X.shape[2] == 1:
      sig = []
      for x in X[:,:,0]:
        fx = np.absolute(np.fft.fft(x))
        fc = np.mean(fx)
        sig.append([fc, rms(fx), rms(fx-fc)])
      return np.array(sig)
    de = []
    for x in X[:,:,0]:
      fx = np.absolute(np.fft.fft(x))
      fc = np.mean(fx)
      de.append([fc, rms(fx), rms(fx-fc)])
    de = np.array(de)
    fe = []
    for x in X[:,:,1]:
      fx = np.absolute(np.fft.fft(x))
      fc = np.mean(fx)
      fe.append([fc, rms(fx), rms(fx-fc)])
    fe = np.array(fe)
    return np.concatenate((de,fe),axis=1)

class Statistical(TransformerMixin):
  def __init__(self):
    pass
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    st = StatisticalTime()
    stfeats = st.transform(X)
    sf = StatisticalFrequency()
    sffeats = sf.transform(X)
    return np.concatenate((stfeats,sffeats),axis=1)
   
import pywt
class WaveletPackage(TransformerMixin):
  def __init__(self):
    pass
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    def Energy(coeffs, k):
      return np.sqrt(np.sum(np.array(coeffs[-k]) ** 2)) / len(coeffs[-k])
    def getEnergy(wp):
      coefs = np.asarray([n.data for n in wp.get_leaf_nodes(True)])
      return np.asarray([Energy(coefs,i) for i in range(2**wp.maxlevel)])
    if X.shape[2] == 1:
      return np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4', mode='symmetric', maxlevel=4)) for x in X[:,:,0]])
    de = np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4', mode='symmetric', maxlevel=4)) for x in X[:,:,0]])
    fe = np.array([getEnergy(pywt.WaveletPacket(data=x, wavelet='db4', mode='symmetric', maxlevel=4)) for x in X[:,:,1]])
    return np.concatenate((de,fe),axis=1)

class Heterogeneous(TransformerMixin):
  def __init__(self):
    pass
  def fit(self, X, y=None):
    return self
  def transform(self, X, y=None):
    st = StatisticalTime()
    stfeats = st.transform(X)
    sf = StatisticalFrequency()
    sffeats = sf.transform(X)
    wp = WaveletPackage()
    wpfeats = wp.transform(X)
    return np.concatenate((stfeats,sffeats,wpfeats),axis=1)

"""#Define classification models"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

"""K-NN"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

param_dist = {"n_neighbors": [1, 3, 5, 7, 9, 11, 13, 15]}
knn = Pipeline([('FeatureExtraction', Heterogeneous()),
                ('scaler', StandardScaler()),
                ('KNN', GridSearchCV(KNeighborsClassifier(),
                                     param_grid=param_dist))])

"""SVM"""

from sklearn.svm import SVC
param_dist = {'SVM__C':[0.001, 0.01, 0.1, 1, 10], 'SVM__gamma':[0.001, 0.01, 0.1, 1]}
svm = Pipeline([('FeatureExtraction', Heterogeneous()),
                ('scaler', StandardScaler()),
                ('SVM', GridSearchCV(SVC(),
                                     param_grid=param_dist))])

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

param_dist = {"n_estimators": [10, 20, 50],
              "max_features": [8, None]}
rf = Pipeline([('FeatureExtraction', Heterogeneous()),
               ('scaler', StandardScaler()),
               ('RF', GridSearchCV(RandomForestClassifier(),
                                   param_grid=param_dist))])

"""Artificial Neural Network - Conv1D"""

from keras import backend as K
def f1_score_macro(y_true,y_pred):
    def recall(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall
    def precision(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))

from keras import layers
from keras import Input
from keras.models import Model
from keras.callbacks import EarlyStopping,ReduceLROnPlateau
from keras.utils import to_categorical

from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils import shuffle
class ANNConv1D(BaseEstimator, ClassifierMixin):
  def __init__(self, shape=xn_0.shape, filters=4, kernel_size=7):
    self.shape = shape
    self.filters = filters
    self.kernel_size = kernel_size
  def fit(self, X, y=None):
    y_cat = to_categorical(y)
    signal_input = Input(shape=(self.shape[1],self.shape[-1]), dtype='float32', name='signal')
    x = layers.Conv1D(self.filters, self.kernel_size, activation='relu', name='conv1d_1')(signal_input)
    x = layers.MaxPooling1D(self.kernel_size, name='max_pooling1d_1')(x)
    x = layers.Flatten(name='flatten')(x)
    condition_output = layers.Dense(len(labels_dict),activation='softmax',name='condition')(x)
    self.model = Model(signal_input, condition_output) 
    self.model.compile(optimizer='rmsprop',
                       loss='mean_squared_error', 
                       metrics=['accuracy',f1_score_macro])
    verbose = 0
    if debug:
      verbose = 1      
    self.history = self.model.fit(X ,y_cat, epochs=100, 
                                  validation_split=0.2,
                                  callbacks=[EarlyStopping(patience=3),
                                             ReduceLROnPlateau()],
                                  verbose=verbose)
    return self
  def predict(self, X, y=None):
    return np.argmax(self.model.predict(X), axis=1)

conv1d = ANNConv1D(xn_0.shape)
from sklearn.model_selection import StratifiedShuffleSplit
param_dist = {"filters": [16, 32, 64],
              "kernel_size": [32, 64, 128]}
conv1d = GridSearchCV(conv1d, param_grid=param_dist, 
                      cv=StratifiedShuffleSplit(n_splits=1, test_size=0.1))

"""List of classification methods"""

clfs = [("K-NNeighbors", knn),
        ("SVM", svm),
        ("RandomForest", rf),
        ("ANN-Conv1d", conv1d)]

"""#Perform experiments."""

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.metrics import f1_score,accuracy_score
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from sklearn.exceptions import ConvergenceWarning
warnings.simplefilter(action='ignore', category=ConvergenceWarning)
from sklearn.exceptions import UndefinedMetricWarning
warnings.simplefilter(action='ignore', category=UndefinedMetricWarning)
#Confunsion Matrix Configuration
genconfmat = True
if genconfmat:
  class_names = []
  for label in labels_dict.keys():
    class_names.append(label)
  figprop = np.linspace(1,0.5,25)
  tam = len(class_names)*figprop[len(class_names)]

results = {}
models = {}
nrounds = 1 if debug else 5

"""##kfold Experiments"""

X,y = None,None
for load in range(4):
  X,y = concatenate_datasets(X,y,eval('xn_'+str(load)),eval('yn_'+str(load)))
for severity in severities: 
  X,y = concatenate_datasets(X,y,eval('x'+str(severity)),eval('y'+str(severity)))
rskf = RepeatedStratifiedKFold(n_splits=len(severities), n_repeats=nrounds,
                               random_state=36851234)
fold = 0
count_round = 0

results['kfold'] = {}
models['kfold'] = {}
y_test_round = None
y_pred_round = {}

print("k-Fold")
for train_index, test_index in rskf.split(X, y):
  print("{}/{}".format(fold+1,rskf.get_n_splits()//nrounds), end=" x ")
  x_train, x_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  if y_test_round is None:
    y_test_round = np.copy(y_test)
  else:
    y_test_round = np.concatenate((y_test_round,y_test))
  j = count_round//(rskf.get_n_splits()//nrounds)
  count_round += 1
  print("{}/{}".format(j+1,nrounds))
  
  print(x_train)
  print(y_train)
  
                             #Experiments#
###############################################################################

  for clfname,model in clfs:
    print(clfname, end=":\t")
    if not clfname in results['kfold']:
      results['kfold'][clfname] = []
    history = model.fit(x_train ,y_train)
    y_pred = model.predict(x_test)
    if not clfname in y_pred_round:
      y_pred_round[clfname] = None
    if y_pred_round[clfname] is None:
      y_pred_round[clfname] = np.copy(y_pred)
    else:
      y_pred_round[clfname] = np.concatenate((y_pred_round[clfname],y_pred))
    if not clfname+str(fold) in models['kfold']:
      models['kfold'][clfname+str(fold)] = model
    results['kfold'][clfname].append([accuracy_score(y_test,y_pred),f1_score(y_test,y_pred,average='macro')])
    print(results['kfold'][clfname][-1])
    if genconfmat:
      cnf_matrix = confusion_matrix(y_test, y_pred)
      print(cnf_matrix)
      plt.figure(figsize=(tam,tam))
      plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Fold '+str(fold+1)+' - Round '+str(j+1), normalize=False)
      plt.savefig('cnfmatrix_kfold'+clfname+str(fold)+'round'+str(j)+'.png')
      plt.show()
  if fold >= (rskf.get_n_splits()//nrounds)-1:
    fold = 0
    for clfname,model in clfs:
      print([accuracy_score(y_test_round, y_pred_round[clfname]),
             f1_score(y_test_round, y_pred_round[clfname],average='macro')])
      if genconfmat:
        cnf_matrix = confusion_matrix(y_test_round, y_pred_round[clfname])
        print(cnf_matrix)
        plt.figure(figsize=(tam,tam))
        plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Round '+str(j+1), normalize=False)
        plt.savefig('cnfmatrix_kfold'+clfname+'_round'+str(j+1)+'.png')        
        plt.show()
      y_pred_round[clfname] = None
    y_test_round = None
  else:
    fold += 1
  if debug:
    break

"""## byload Experiments"""

X,y = None,None
for load in range(4):
  X,y = concatenate_datasets(X,y,eval('xn_'+str(load)),eval('yn_'+str(load)))
rskf = RepeatedStratifiedKFold(n_splits=len(severities), n_repeats=nrounds,
                               random_state=36851234)
fold = 0
count_round = 0

results['kfold'] = {}
models['kfold'] = {}
y_test_round = None
y_pred_round = {}

print("k-Fold")
for train_index, test_index in rskf.split(X, y):
  print("{}/{}".format(fold+1,rskf.get_n_splits()//nrounds), end=" x ")
  x_train, x_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]
  if y_test_round is None:
    y_test_round = np.copy(y_test)
  else:
    y_test_round = np.concatenate((y_test_round,y_test))
  j = count_round//(rskf.get_n_splits()//nrounds)
  count_round += 1
  print("{}/{}".format(j+1,nrounds))
  
  print(x_train)
  print(y_train)
  
                             #Experiments#
###############################################################################

  for clfname,model in clfs:
    print(clfname, end=":\t")
    if not clfname in results['kfold']:
      results['kfold'][clfname] = []
    history = model.fit(x_train ,y_train)
    y_pred = model.predict(x_test)
    if not clfname in y_pred_round:
      y_pred_round[clfname] = None
    if y_pred_round[clfname] is None:
      y_pred_round[clfname] = np.copy(y_pred)
    else:
      y_pred_round[clfname] = np.concatenate((y_pred_round[clfname],y_pred))
    if not clfname+str(fold) in models['kfold']:
      models['kfold'][clfname+str(fold)] = model
    results['kfold'][clfname].append([accuracy_score(y_test,y_pred),f1_score(y_test,y_pred,average='macro')])
    print(results['kfold'][clfname][-1])
    if genconfmat:
      cnf_matrix = confusion_matrix(y_test, y_pred)
      print(cnf_matrix)
      plt.figure(figsize=(tam,tam))
      plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Fold '+str(fold+1)+' - Round '+str(j+1), normalize=False)
      plt.savefig('cnfmatrix_kfold'+clfname+str(fold)+'round'+str(j)+'.png')
      plt.show()
  if fold >= (rskf.get_n_splits()//nrounds)-1:
    fold = 0
    for clfname,model in clfs:
      print([accuracy_score(y_test_round, y_pred_round[clfname]),
             f1_score(y_test_round, y_pred_round[clfname],average='macro')])
      if genconfmat:
        cnf_matrix = confusion_matrix(y_test_round, y_pred_round[clfname])
        print(cnf_matrix)
        plt.figure(figsize=(tam,tam))
        plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Round '+str(j+1), normalize=False)
        plt.savefig('cnfmatrix_kfold'+clfname+'_round'+str(j+1)+'.png')        
        plt.show()
      y_pred_round[clfname] = None
    y_test_round = None
  else:
    fold += 1
  if debug:
    break

"""##byseverity Experiments"""

from sklearn.utils import shuffle
results['byseverity'] = {}
models['byseverity'] = {}
y_test_round = None
y_pred_round = {}

print("By Severity")
for j in range(nrounds):
  for i,fold in enumerate(severities):
    print("{}".format(fold), end=" x ")
    x_test,y_test = eval('xn_'+str((i+len(severities)+1)%4)),eval('yn_'+str((i+len(severities)+1)%4))
    x_test,y_test = concatenate_datasets(x_test,y_test,eval('x'+str(fold)),eval('y'+str(fold)))
    if y_test_round is None:
      y_test_round = np.copy(y_test)
    else:
      y_test_round = np.concatenate((y_test_round,y_test))
    x_train,y_train = None,None  
    for ni in list(range(0,4)):
      if ni != (i+len(severities)+1)%4:
        x_train,y_train = concatenate_datasets(x_train,y_train,eval('xn_'+str(ni)),eval('yn_'+str(ni)))
    for tfold in severities[:i]+severities[i+1:]:
      x_train,y_train = concatenate_datasets(x_train,y_train,eval('x'+str(tfold)),eval('y'+str(tfold)))
    x_train,y_train = shuffle(x_train,y_train,random_state=36851234+j)
    print("{}/{}".format(j+1,nrounds))
    for clfname,model in clfs:
      print(clfname, end=":\t")
      if not clfname in results['byseverity']:
        results['byseverity'][clfname] = []
      history = model.fit(x_train ,y_train)
      y_pred = model.predict(x_test)
      if not clfname in y_pred_round:
        y_pred_round[clfname] = None
      if y_pred_round[clfname] is None:
        y_pred_round[clfname] = np.copy(y_pred)
      else:
        y_pred_round[clfname] = np.concatenate((y_pred_round[clfname],y_pred))
      if not clfname+str(fold) in models['byseverity']:
        models['byseverity'][clfname+str(fold)] = model
      results['byseverity'][clfname].append([accuracy_score(y_test,y_pred),f1_score(y_test,y_pred,average='macro')])
      print(results['byseverity'][clfname][-1])
      if genconfmat:
        cnf_matrix = confusion_matrix(y_test, y_pred)
        print(cnf_matrix)
        plt.figure(figsize=(tam,tam))
        plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Fold '+str(fold)+' - Round '+str(j+1), normalize=False)
        plt.savefig('cnfmatrix_byseverity'+clfname+str(fold)+'round'+str(j)+'.png')
        plt.show()
        
                                     #Experiments#
###############################################################################
        
  for clfname,model in clfs:
    print([accuracy_score(y_test_round, y_pred_round[clfname]),
            f1_score(y_test_round, y_pred_round[clfname],average='macro')])
    if genconfmat:
      cnf_matrix = confusion_matrix(y_test_round, y_pred_round[clfname])
      print(cnf_matrix)
      plt.figure(figsize=(tam,tam))
      plot_confusion_matrix(cnf_matrix, classes=class_names, title=clfname+' - Round '+str(j+1), normalize=False)
      plt.savefig('cnfmatrix_kfold'+clfname+'_round'+str(j+1)+'.png')
      plt.show()
    y_pred_round[clfname] = None
  y_test_round = None

"""#Results Summary"""

for evaluation in results.keys():
  print("\n"+30*"#"+"\n"+evaluation+"\n"+30*"#")
  for clfname,model in clfs:
    print("\n\t"+clfname+" Results\nFold\tAccuracy\tF1-Score")
    for i,r in enumerate(results[evaluation][clfname]):
      print("{}\t".format(i+1),end="")
      print(r)
    print("Average\tAccuracy\tF1-Score\n\t",end="")
    print(np.mean(results[evaluation][clfname],axis=0))
    print("StdDev\tAccuracy\tF1-Score\n\t",end="")
    print(np.std(results[evaluation][clfname],axis=0))